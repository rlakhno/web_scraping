{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce85f853",
   "metadata": {},
   "source": [
    "# LinkedIn Web Scraping by Selenium\n",
    "\n",
    "The following code used Selenium webdriver to perform the interactive data access and scraping with credentials, and button clicking by programming, in which a wait time was applied\n",
    "\n",
    "**Nov 18, 2023**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7ca84c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import time\n",
    "import pandas as pd    \n",
    "# ------------- # \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (119.0.6045.105) detected in PATH at C:\\Windows\\chromedriver.exe might not be compatible with the detected chrome version (120.0.6099.225); currently, chromedriver 120.0.6099.109 is recommended for chrome 120.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/login');\n",
    "time.sleep(2)\n",
    "\n",
    "# Maximize Window\n",
    "# driver.maximize_window() \n",
    "# driver.minimize_window()  \n",
    "driver.maximize_window()  \n",
    "driver.switch_to.window(driver.current_window_handle)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4738eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Credentials\n",
    "# Reading txt file where we have our user credentials\n",
    "# with open('user_credentials.txt', 'r',encoding=\"utf-8\") as file:\n",
    "#     user_credentials = file.readlines()\n",
    "#     user_credentials = [line.rstrip() for line in user_credentials]\n",
    "\n",
    "user_name =  \"rouslanlakhno@gmail.com\" # credential\n",
    "password =  \"r7r7r7r7r7\" # credential\n",
    "driver.find_element(\"xpath\",'//*[@id=\"username\"]').send_keys(user_name)\n",
    "driver.find_element(\"xpath\",'//*[@id=\"password\"]').send_keys(password)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Login button\n",
    "driver.find_element(\"xpath\",'//*[@id=\"organic-div\"]/form/div[3]/button').click()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d3080def",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Data Analyst'\n",
    "location = 'Canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9a4abe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to search results directly\n",
    "\n",
    "driver.get(\"https://www.linkedin.com/jobs/search/?keywords=Data%20Analyst&location=Canada\")\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "139dd7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links are being collected now.\n",
      "Collecting the links in the page: 1\n",
      "Found 25 links for job offers\n",
      "Found https://www.linkedin.com/jobs/view/3805738088/?eBP=CwEAAAGNKEWWo5dn4yzl9g3Gbn8W792NmEQQRXKbF6-w8Lr9cqAueG-TJXax0wnIHzFHz0ga4N743HDEGSsmRyNNb-g5iMy_0PVkHj3f2MFRgZf5PTSJqCs8gtsXSUvuYlTZH15tXa1sRYcGrHeMndND-5WROEGrKlWaGE6TJPy1PplQ2_n_dEgovvrh_U5Nni3KVPtBv_Q4o6mVLH_mhGlDrgih8cRzyi56lLb-p4bixgj4XrME88hFf-zXouw_Zk3e5HeBLhT_36rDoWASwqm8HCJdsKhZybwX7aG3KrxusTYzzLR2mEa0keULS6TUlhxO-QPht17ghSU-oxsz1ZWw1GjA0AwFYFRhBwNkOzvv0NQhVaajRBfcSpF-I0Vm_OydZ_T57jA&refId=GNWK2FNdkH7pvtV%2BATYEXg%3D%3D&trackingId=0WlpNgbry26LAMMwk%2FB5zA%3D%3D&trk=flagship3_search_srp_jobs first link\n"
     ]
    }
   ],
   "source": [
    "# Get all links for these offers\n",
    "links = []\n",
    "# Navigate 13 pages\n",
    "print('Links are being collected now.')\n",
    "try: \n",
    "    # for page in range(2,14):       -------------Rouslan \n",
    "    for page in range(2,3):    \n",
    "        time.sleep(2)\n",
    "        jobs_block = driver.find_element(By.CLASS_NAME, 'jobs-search-results-list')\n",
    "        jobs_list= jobs_block.find_elements(By.CSS_SELECTOR, '.jobs-search-results__list-item')\n",
    "\n",
    "        for job in jobs_list:\n",
    "            all_links = job.find_elements(By.TAG_NAME, 'a')\n",
    "            for a in all_links:\n",
    "                if str(a.get_attribute('href')).startswith(\"https://www.linkedin.com/jobs/view\") and a.get_attribute('href') not in links: \n",
    "                    links.append(a.get_attribute('href'))\n",
    "                else:\n",
    "                    pass\n",
    "            # scroll down for each job element\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "\n",
    "        print(f'Collecting the links in the page: {page-1}')\n",
    "        # go to next page:\n",
    "        driver.find_element(By.XPATH, f\"//button[@aria-label='Page {page}']\").click()\n",
    "        time.sleep(3)\n",
    "except:\n",
    "    pass\n",
    "print('Found ' + str(len(links)) + ' links for job offers')\n",
    "print('Found ' + str(links[0]) + ' first link')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e93b3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute tokens $x(\"some_xpath\") or $$(\"css-selectors\") in Console \n",
    "# Create empty lists to store information\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_locations = []\n",
    "work_methods = []\n",
    "post_dates = []\n",
    "work_times = [] \n",
    "job_desc = []\n",
    "\n",
    "i = 0\n",
    "j = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7f4a9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the links and collecting information just started.\n",
      "contents [<selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"A4F75A31C438E99436FFA33769DCD4F4_element_1181\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"A4F75A31C438E99436FFA33769DCD4F4_element_1190\")>] ================\n",
      "job_titles ['Quantitative Analyst'] ================\n",
      "company_names ['Affirm'] ==========\n",
      "company_locations ['Affirm · Ottawa, ON · 1 week ago · 86 applicants'] ==========\n",
      "work_methods ['Remote'] ==========\n",
      "post_dates ['1 week ago'] ==========\n",
      "work_times ['Full-time'] ==========\n",
      "Scraping the Job Offer 1 DONE.\n",
      "Scraping the Job Offer 2\n",
      "contents [<selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"34120FEC964EF78508F8108DABB03D63_element_1207\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"34120FEC964EF78508F8108DABB03D63_element_1225\")>] ================\n",
      "job_titles ['Quantitative Analyst', 'Competitive Intelligence Analyst, Falcon for IT (Remote)'] ================\n",
      "company_names ['Affirm', 'CrowdStrike'] ==========\n",
      "company_locations ['Affirm · Ottawa, ON · 1 week ago · 86 applicants', 'CrowdStrike · Nova Scotia, Canada · Reposted 1 week ago · 79 applicants'] ==========\n",
      "work_methods ['Remote', 'Remote'] ==========\n",
      "post_dates ['1 week ago', 'Reposted 1 week ago'] ==========\n",
      "work_times ['Full-time', 'Full-time'] ==========\n",
      "Scraping the Job Offer 2 DONE.\n",
      "Scraping the Job Offer 3\n",
      "contents [<selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"49DF9FA1E300EEB7E88F005396490C3E_element_1244\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4fb765c59eba1f7c9e1fcdc22d595c53\", element=\"49DF9FA1E300EEB7E88F005396490C3E_element_1267\")>] ================\n",
      "job_titles ['Quantitative Analyst', 'Competitive Intelligence Analyst, Falcon for IT (Remote)', 'ICQA Data Analyst'] ================\n",
      "company_names ['Affirm', 'CrowdStrike', 'Amazon'] ==========\n",
      "company_locations ['Affirm · Ottawa, ON · 1 week ago · 86 applicants', 'CrowdStrike · Nova Scotia, Canada · Reposted 1 week ago · 79 applicants', 'Amazon · Hamilton, ON · Reposted 2 days ago · 77 applicants'] ==========\n",
      "work_methods ['Remote', 'Remote', 'On-site\\nMatches your job preferences, workplace type is On-site.'] ==========\n",
      "post_dates ['1 week ago', 'Reposted 1 week ago', 'Reposted 2 days ago'] ==========\n",
      "work_times ['Full-time', 'Full-time', 'On-site'] ==========\n",
      "Scraping the Job Offer 3 DONE.\n",
      "Scraping the Job Offer 4\n"
     ]
    }
   ],
   "source": [
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "# for i in range(len(links)):   -------------Rouslan\n",
    "for i in range(0,3):\n",
    "    try:\n",
    "        driver.get(links[i])\n",
    "        i=i+1\n",
    "        time.sleep(2)\n",
    "        # Click See more.\n",
    "        driver.find_element(By.XPATH, f\"//button[@aria-label='Click to see more description']\").click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Find the general information of the job offers\n",
    "    contents = driver.find_elements(By.CLASS_NAME,'p5')\n",
    "    print('contents ' + str(contents) + ' ================')\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # job_titles.append(content.find_element(By.TAG_NAME,\"h1\").text) ------Rouslan\n",
    "            # job_titles.append(content.find_element(By.XPATH, f\"/html/body/div[@class='application-outlet']/div[@class='authentication-outlet']/div[2]/div/div//div[@role='main']//h1[1]\").text)\n",
    "            job_titles.append(content.find_element(By.CSS_SELECTOR, \".t-24\").text)\n",
    "            print('job_titles ' + str(job_titles) + ' ================')\n",
    "            time.sleep(2)\n",
    "\n",
    "            company_names.append(content.find_element(By.CSS_SELECTOR,\".job-details-jobs-unified-top-card__primary-description-without-tagline > .app-aware-link\").text)\n",
    "            print('company_names ' + str(company_names) + ' ==========')\n",
    "            company_locations.append(content.find_element(By.CSS_SELECTOR,\".job-details-jobs-unified-top-card__primary-description-without-tagline\").text)\n",
    "            print('company_locations ' + str(company_locations) + ' ==========')\n",
    "            work_methods.append(content.find_element(By.CSS_SELECTOR,\".job-details-jobs-unified-top-card__job-insight:nth-child(1) > span > span:nth-child(1)\").text)\n",
    "            print('work_methods ' + str(work_methods) + ' ==========')\n",
    "            post_dates.append(content.find_element(By.CSS_SELECTOR,\".tvm__text:nth-child(4)\").text)\n",
    "            print('post_dates ' + str(post_dates) + ' ==========')\n",
    "            work_times.append(content.find_element(By.CSS_SELECTOR,\".ui-label > span:nth-child(1)\").text)\n",
    "            print('work_times ' + str(work_times) + ' ==========')\n",
    "            print(f'Scraping the Job Offer {j} DONE.')\n",
    "            j+= 1\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Scraping the job description\n",
    "    job_description = driver.find_elements(By.CLASS_NAME,'jobs-description__content')\n",
    "    for description in job_description:\n",
    "        job_text = description.find_element(By.CLASS_NAME,\"jobs-box__html-content\").text\n",
    "        job_desc.append(job_text)\n",
    "        # print('job_desc ' + str(job_desc) + ' ================')\n",
    "        print(f'Scraping the Job Offer {j}')\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        time.sleep(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2e54e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_location</th>\n",
       "      <th>work_method</th>\n",
       "      <th>post_date</th>\n",
       "      <th>work_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [job_title, company_name, company_location, work_method, post_date, work_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe \n",
    "df = pd.DataFrame(list(zip(job_titles,company_names,\n",
    "                    company_locations,work_methods,\n",
    "                    post_dates,work_times)),\n",
    "                    columns =['job_title', 'company_name',\n",
    "                           'company_location','work_method',\n",
    "                           'post_date','work_time'])\n",
    "# df.job_title = 'job_titles'\n",
    "# Storing the data to csv file\n",
    "df.to_csv(r'C:\\Users\\rousl\\OneDrive\\Documents\\Merto College\\Data science work experience project\\Web Scraping\\job_offers.csv', index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6fe33ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Visit each link one by one to scrape the information\\nprint(\\'Visiting the links and collecting information just started.\\')\\n# for i in range(len(links)):   -------------Rouslan\\nfor i in range(0,2):\\n    try:\\n        driver.get(links[i])\\n        i=i+1\\n        time.sleep(2)\\n        # Click See more.\\n        driver.find_element(By.XPATH, f\"//button[@aria-label=\\'Click to see more description\\']\").click()\\n        time.sleep(2)\\n    except:\\n        pass\\n    \\n    # Find the general information of the job offers\\n    contents = driver.find_elements(By.CLASS_NAME,\\'p5\\')\\n    print(\\'contents \\' + str(contents) + \\' ================\\')\\n    for content in contents:\\n        try:\\n            # job_titles.append(content.find_element(By.TAG_NAME,\"h1\").text) ------Rouslan\\n            # job_titles.append(content.find_element(By.TAG_NAME,\"t-24 t-bold job-details-jobs-unified-top-card__job-title\").text)\\n            job_titles.append(content.find_element(By.XPATH, f\"/html/body/div[@class=\\'application-outlet\\']/div[@class=\\'authentication-outlet\\']/div[2]/div/div//div[@role=\\'main\\']//h1[1]\").text)\\n            print(\\'job_titles \\' + str(job_titles) + \\' ================\\')\\n            time.sleep(2)\\n            # company_names.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__company-name\").text)\\n            company_names.append(content.find_element(By.CLASS_NAME,\".job-details-jobs-unified-top-card__primary-description-without-tagline.mb2\").text)\\n            print(\\'company_names \\') \\n            print(\\'company_names \\' + company_names + \\' ==========\\')\\n            company_locations.append(content.find_element(By.CLASS_NAME,\".job-details-jobs-unified-top-card__primary-description-without-tagline.mb2\").text)\\n            \\n            print(\\'company_locations \\' + str(company_locations) + \\' ==========\\')\\n\\n            work_methods.append(content.find_element(By.CLASS_NAME,\"job-details-jobs-unified-top-card__job-insight\").text)\\n            print(\\'work_methods \\' + str(work_methods) + \\' ==========\\')\\n\\n            post_dates.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__posted-date\").text)\\n            \\n            work_times.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__job-insight\").text)\\n            print(f\\'Scraping the Job Offer {j} DONE.\\')\\n            j+= 1\\n        except:\\n            pass\\n        time.sleep(2)\\n                \\n        # Scraping the job description\\n    job_description = driver.find_elements(By.CLASS_NAME,\\'jobs-description__content\\')\\n    for description in job_description:\\n        job_text = description.find_element(By.CLASS_NAME,\"jobs-box__html-content\").text\\n        job_desc.append(job_text)\\n        print(f\\'Scraping the Job Offer {j}\\')\\n        time.sleep(2)  \\n      '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "# for i in range(len(links)):   -------------Rouslan\n",
    "for i in range(0,2):\n",
    "    try:\n",
    "        driver.get(links[i])\n",
    "        i=i+1\n",
    "        time.sleep(2)\n",
    "        # Click See more.\n",
    "        driver.find_element(By.XPATH, f\"//button[@aria-label='Click to see more description']\").click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Find the general information of the job offers\n",
    "    contents = driver.find_elements(By.CLASS_NAME,'p5')\n",
    "    print('contents ' + str(contents) + ' ================')\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # job_titles.append(content.find_element(By.TAG_NAME,\"h1\").text) ------Rouslan\n",
    "            # job_titles.append(content.find_element(By.TAG_NAME,\"t-24 t-bold job-details-jobs-unified-top-card__job-title\").text)\n",
    "            job_titles.append(content.find_element(By.XPATH, f\"/html/body/div[@class='application-outlet']/div[@class='authentication-outlet']/div[2]/div/div//div[@role='main']//h1[1]\").text)\n",
    "            print('job_titles ' + str(job_titles) + ' ================')\n",
    "            time.sleep(2)\n",
    "            # company_names.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__company-name\").text)\n",
    "            company_names.append(content.find_element(By.CLASS_NAME,\".job-details-jobs-unified-top-card__primary-description-without-tagline.mb2\").text)\n",
    "            print('company_names ') \n",
    "            print('company_names ' + company_names + ' ==========')\n",
    "            company_locations.append(content.find_element(By.CLASS_NAME,\".job-details-jobs-unified-top-card__primary-description-without-tagline.mb2\").text)\n",
    "            \n",
    "            print('company_locations ' + str(company_locations) + ' ==========')\n",
    "\n",
    "            work_methods.append(content.find_element(By.CLASS_NAME,\"job-details-jobs-unified-top-card__job-insight\").text)\n",
    "            print('work_methods ' + str(work_methods) + ' ==========')\n",
    "\n",
    "            post_dates.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__posted-date\").text)\n",
    "            \n",
    "            work_times.append(content.find_element(By.CLASS_NAME,\"jobs-unified-top-card__job-insight\").text)\n",
    "            print(f'Scraping the Job Offer {j} DONE.')\n",
    "            j+= 1\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "                \n",
    "        # Scraping the job description\n",
    "    job_description = driver.find_elements(By.CLASS_NAME,'jobs-description__content')\n",
    "    for description in job_description:\n",
    "        job_text = description.find_element(By.CLASS_NAME,\"jobs-box__html-content\").text\n",
    "        job_desc.append(job_text)\n",
    "        print(f'Scraping the Job Offer {j}')\n",
    "        time.sleep(2)  \n",
    "      '''      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e2fe5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating the dataframe \n",
    "df = pd.DataFrame(list(zip(job_titles,company_names,\n",
    "                    company_locations,work_methods,\n",
    "                    post_dates,work_times)),\n",
    "                    columns =['job_title', 'company_name',\n",
    "                           'company_location','work_method',\n",
    "                           'post_date','work_time'])\n",
    "\n",
    "# Storing the data to csv file\n",
    "df.to_csv(r'C:\\Users\\rousl\\OneDrive\\Documents\\Merto College\\Data science work experience project\\Web Scraping\\job_offers.csv', index=False)\n",
    "\n",
    "# Output job descriptions to txt file\n",
    "with open(r'C:\\Users\\rousl\\OneDrive\\Documents\\Merto College\\Data science work experience project\\Web Scraping\\job_descriptions.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in job_desc:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5d1ce00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_location</th>\n",
       "      <th>work_method</th>\n",
       "      <th>post_date</th>\n",
       "      <th>work_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [job_title, company_name, company_location, work_method, post_date, work_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
