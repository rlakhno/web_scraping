{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5328e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rousl\\anaconda3\\lib\\site-packages (4.15.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rousl\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\rousl\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a53f5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service \u001b[38;5;28;01mas\u001b[39;00m ChromeService \n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwebdriver_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChromeDriverManager \n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7617f3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m options\u001b[38;5;241m.\u001b[39mheadless \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# inisiate driver \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mChromeService(\u001b[43mChromeDriverManager\u001b[49m()\u001b[38;5;241m.\u001b[39minstall()), options\u001b[38;5;241m=\u001b[39moptions) \n\u001b[0;32m     11\u001b[0m links\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#driver=webdriver.Chrome()\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " \n",
    "# instantiate options \n",
    "options = webdriver.ChromeOptions() \n",
    " \n",
    "# run browser in headless mode \n",
    "options.headless = True \n",
    " \n",
    "# inisiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options) \n",
    "links=[]\n",
    "#driver=webdriver.Chrome()\n",
    "df=pd.DataFrame(columns=[\"Title\", \"Company\", \"Location\", \"Job_Type\", \"Salary\", \"Posting_Date\",\"Current_date\", \n",
    "                         \"Description\"])\n",
    "''' driver.get(url)\n",
    "no_of_jobs=driver.find_element(By.CLASS_NAME,value='jobsearch-JobCountAndSortPane-jobCount css-13jafh6 eu4oa1w0')\n",
    "print(no_of_jobs)\n",
    "x=no_of_jobs/15\n",
    "for i in x: '''\n",
    "for i in range(0,500,10):\n",
    "    url=\"https://ca.indeed.com/jobs?q=data+analyst&l=canada&start=\"+str(i) \n",
    "    driver.get(url)    \n",
    "\n",
    "    driver.implicitly_wait(4)\n",
    "    all_jobs=driver.find_elements(By.CLASS_NAME,value='job_seen_beacon')\n",
    "    for job in all_jobs:\n",
    "        result_html=job.get_attribute(\"innerHTML\")\n",
    "        soup=BeautifulSoup(result_html,\"html.parser\")\n",
    "        try:\n",
    "                title=job.find_element(By.TAG_NAME, 'span').text\n",
    "        except:\n",
    "                title='None'\n",
    "        try:\n",
    "                company=soup.find(\"span\", class_=\"css-1x7z1ps eu4oa1w0\").text\n",
    "        except:\n",
    "                company='None'   \n",
    "\n",
    "        try:\n",
    "                location=soup.find(class_=\"css-t4u72d eu4oa1w0\").text.replace(\"\\n\",\" \")\n",
    "        except:\n",
    "                location='None'\n",
    "\n",
    "        try:\n",
    "                job_type=soup.find(class_=\"css-l1havw2 eu4oa1w0\").text.replace(\"\\n\",\" \")\n",
    "        except:\n",
    "                job_type='None'\n",
    "\n",
    "        try:\n",
    "                salary=soup.find(\"div\",class_=\"metadata salary-snippet-container\").text.replace(\"\\n\",\" \")\n",
    "        except:\n",
    "                salary='None'\n",
    "\n",
    "        ''' try: \n",
    "                postdate = soup.find('span','date').text\n",
    "        except:\n",
    "                postdate='None' '''\n",
    "        try:\n",
    "            postdate=job.find_element(By.CLASS_NAME, value='date').get_attribute('textContent')\n",
    "        except:\n",
    "            postdate='None'\n",
    "\n",
    "        currentdate=datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        link=soup.find('a',class_='jcs-JobTitle').get('href')\n",
    "        joblink = 'https://ca.indeed.com' + link\n",
    "        try:\n",
    "               description=soup.find(class_=\"job-snippet\").text.replace(\"\\n\",\" \")\n",
    "        except:\n",
    "               description='None'  \n",
    "        links.append(joblink)\n",
    "        try:\n",
    "            close_button = driver.find_element(By.CLASS_NAME, value='css-yi9ndv e8ju0x51')\n",
    "            sleep(1)\n",
    "            close_button.click() \n",
    "        except:\n",
    "            print('\\n')\n",
    "\n",
    "        df=df.append({\"Title\":title,\"Company\":company,\"Location\":location, \"Job_Type\":job_type, \"Salary\":salary,\"Posting_Date\":postdate,\n",
    "                             \"Current_date\":currentdate,\"Description\":description}, ignore_index=True)\n",
    "        time.sleep(15)  \n",
    "        \n",
    "           \n",
    "            \n",
    "df.to_csv(\"jobs_details.csv\")\n",
    "print('Found ' + str(len(links)) + ' links for job offers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "14a82b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4106818358.py, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [85]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('*****************************\\n')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create empty lists to store information\n",
    "job_desc = []\n",
    "job_title=[]\n",
    "i = 0\n",
    "\n",
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "for i in range(len(links)):\n",
    "    try:\n",
    "        driver.get(links[i])\n",
    "        i=i+1\n",
    "        time.sleep(2)\n",
    "        title=driver.find_element(By.CLASS_NAME, 'jobsearch-JobInfoHeader-title').text\n",
    "        job_title.append(title)\n",
    "        \n",
    "        # Click See more.\n",
    "        driver.find_element(By.XPATH,value='//div[@id=\"jobDescriptionText\"]').click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Scraping the job description\n",
    "    job_description = driver.find_elements(By.ID,'jobDescriptionText')\n",
    "    for description in job_description:\n",
    "        #job_text = description.find_element(By.XPATH,\"//div/ul/li[*]\").text\n",
    "        #job_desc.append(job_text)\n",
    "        \n",
    "        elem = driver.find_element(By.XPATH,value='//div/ul')\n",
    "\n",
    "        all_li = elem.find_elements(By.TAG_NAME,value=\"li\")\n",
    "        title=driver.find_element(By.CLASS_NAME, 'jobsearch-JobInfoHeader-title').text\n",
    "        job_title.append(title)\n",
    "        \n",
    "        #print(f'Scraping the Job Offer {i}')\n",
    "        for li in all_li:\n",
    "            \n",
    "            #title=li.find('h1').text\n",
    "            text = li.text\n",
    "            print (text)\n",
    "            \n",
    "            job_desc.append(text)\n",
    "with open('job_desc.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for jtitle in job_title:\n",
    "        f.write('\\n')\n",
    "        f.write(jtitle +\": \"+\"\\n\")\n",
    "            \n",
    "        for line in job_desc:\n",
    "            f.write(line)\n",
    "            f.write('.')\n",
    "            f.write(' ')\n",
    "        time.sleep(2) \n",
    "                     \n",
    "        print('*****************************\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d820fa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the links and collecting information just started.\n",
      "AI Technology Business Analyst (Part-Time)\n"
     ]
    }
   ],
   "source": [
    "    # Create empty lists to store information\n",
    "''' import itertools as it\n",
    "try:\n",
    "            popup = WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button[class*='css-yi9ndv e8ju0x51']\")))\n",
    "            popup.click()\n",
    "        except TimeoutException as to:\n",
    "            print(to)\n",
    "            \n",
    "            \n",
    "import csv\n",
    "job_desc = []\n",
    "data=[]\n",
    "i = 0\n",
    "\n",
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "for i in range(len(links)):\n",
    "     try:\n",
    "        driver.get(links[i])\n",
    "        i=i+1\n",
    "        time.sleep(2)\n",
    "        # Click See more.\n",
    "        driver.find_element(By.XPATH,value='//div[@id=\"jobDescriptionText\"]').click()\n",
    "        time.sleep(2)\n",
    "     except:\n",
    "        pass\n",
    "\n",
    "    # Scraping the job description\n",
    "    job_description = driver.find_elements(By.ID,'jobDescriptionText')\n",
    "    for description in job_description:\n",
    "        #job_text = description.find_element(By.XPATH,\"//div/ul/li[*]\").text\n",
    "        #job_desc.append(job_text)\n",
    "        \n",
    "        print(title)\n",
    "        elem = driver.find_element(By.XPATH,value='//div/ul')\n",
    "\n",
    "        all_li = elem.find_elements(By.TAG_NAME,value=\"li\")\n",
    "        \n",
    "        #print(f'Scraping the Job Offer {i}')\n",
    "        title=driver.find_element(By.TAG_NAME, value='h1').text\n",
    "        for li in all_li:\n",
    "            \n",
    "            text = li.text\n",
    "                        \n",
    "            job_desc.append(text)\n",
    "            data.append(title)\n",
    "           \n",
    "            #with open('output.csv', 'w') as f:\n",
    "                #csv.writer(f).writerows(it.zip_longest(data, job_desc))\n",
    "            with open(\"test.txt\",\"w\") as fin:\n",
    "                   for e in it.zip_longest(data,job_desc,fillvalue=''):\n",
    "                        for l in job_desc:\n",
    "                             fin.write(\"{:^5} {:^5}\".format(*e)) '''\n",
    "              \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a692170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_descriptions=[]\\n#for i in Links_list:\\nfor i in range(len(links)):\\n    print(i)\\n    \\n    driver.get(links[i])\\n    driver.find_element(By.XPATH,value=\\'//div[@id=\"jobDescriptionText\"]\\').click()\\n    contents = driver.find_elements(By.CLASS_NAME,\\'ul\\')\\n    full_descriptions.append(contents)    \\n\\n\\ndf[\\'Full Descriptions\\'] = full_descriptions\\ndf.to_csv(\"df1.csv\")'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''full_descriptions=[]\n",
    "#for i in Links_list:\n",
    "for i in range(len(links)):\n",
    "    print(i)\n",
    "    \n",
    "    driver.get(links[i])\n",
    "    driver.find_element(By.XPATH,value='//div[@id=\"jobDescriptionText\"]').click()\n",
    "    contents = driver.find_elements(By.CLASS_NAME,'ul')\n",
    "    full_descriptions.append(contents)    \n",
    "\n",
    "\n",
    "df['Full Descriptions'] = full_descriptions\n",
    "df.to_csv(\"df1.csv\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c010627a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\n \\n# instantiate options \\noptions = webdriver.ChromeOptions() \\n \\n# run browser in headless mode \\noptions.headless = True \\n \\n# inisiate driver \\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options) \\nurl=\"https://ca.indeed.com/jobs?q=data+analyst&l=canada\" #&start=\"\" +str(i) \\n#driver=webdriver.Chrome()\\ndf=pd.DataFrame(columns=[\"Title\", \"Company\", \"Location\", \"Job_Type\", \"Salary\", \"Posting_Date\",\"Current_date\",\"JobLink\", \\n                         \"Description\", \"Full_Job_Description\"])\\nfor i in range(0,500,10):\\n    driver.get(url)    \\n\\n    driver.implicitly_wait(4)\\n    all_jobs=driver.find_elements(By.CLASS_NAME,value=\\'job_seen_beacon\\')\\n    for job in all_jobs:\\n            result_html=job.get_attribute(\"innerHTML\")\\n            soup=BeautifulSoup(result_html,\"html.parser\")\\n\\n            try:\\n                title=job.find_element(By.TAG_NAME, \\'span\\').text \\n            except:\\n                title=\\'None\\'\\n\\n            try:\\n                company=soup.find(\"span\", class_=\"css-1x7z1ps eu4oa1w0\").text\\n            except:\\n                company=\\'None\\'   \\n\\n            try:\\n                location=soup.find(class_=\"css-t4u72d eu4oa1w0\").text.replace(\"\\n\",\" \")\\n            except:\\n                location=\\'None\\'\\n\\n            try:\\n                job_type=soup.find(class_=\"css-l1havw2 eu4oa1w0\").text.replace(\"\\n\",\" \")\\n            except:\\n                job_type=\\'None\\'\\n\\n            try:\\n                salary=soup.find(\"div\",class_=\"metadata salary-snippet-container\").text.replace(\"\\n\",\" \")\\n            except:\\n                salary=\\'None\\'\\n\\n            try: \\n                postdate = soup.find(\\'span\\',\\'date\\').text\\n            except:\\n                postdate=\\'None\\'\\n\\n            currentdate=datetime.today().strftime(\"%Y-%m-%d\")\\n                            \\n            link=soup.find(\\'a\\',class_=\\'jcs-JobTitle\\').get(\\'href\\')\\n            joblink = \\'https://ca.indeed.com\\' + link\\n            try:\\n                description=soup.find(class_=\"job-snippet\").text.replace(\"\\n\",\" \")\\n            except:\\n                description=\\'None\\'  \\n\\n            #job_response = requests.get(joblink)\\n            #job_data = job_response.text\\n            #job_soup = BeautifulSoup(job_data, \\'html.parser\\')\\n            #print(job_soup)\\n            #descriptionlink = job.find_element_by_tag_name(\\'a\\').get(\\'href\\')\\n            concatdescriptionlink = description\\n\\n            driver.get(concatdescriptionlink)\\n            job_description_element = concatdescriptionlink.find_element_by_id(\\'jobDescriptionText\\')\\n            job_description = job_description_element.text.strip()\\n            descriptions.append(job_description)\\n\\n\\n            try:\\n                    WebDriverWait(driver, 5).until(EC.visibility_of_element_located(\\n                            (By.CSS_SELECTOR, \"button.popover-x-button-close.icl-CloseButton\"))).click()\\n            except:\\n                    pass\\n\\n            job_description_tag = job_soup.find(\\'div\\',{\\'id\\':\\'jobDescriptionText\\'})\\n            full_job_description = job_description_tag.text if job_description_tag else \"N/A\"    \\n\\n            df=df.append({\"Title\":title,\"Company\":company,\"Location\":location, \"Job_Type\":job_type, \"Salary\":salary,\"Posting_Date\":postdate,\\n                         \"Current_date\":currentdate,\"Description\":description, \"Full_Job_Description\":full_job_description}, ignore_index=True)\\n            time.sleep(15)  \\n\\n    df.to_csv(\"df1.csv\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import requests\n",
    " \n",
    "# instantiate options \n",
    "options = webdriver.ChromeOptions() \n",
    " \n",
    "# run browser in headless mode \n",
    "options.headless = True \n",
    " \n",
    "# inisiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options) \n",
    "url=\"https://ca.indeed.com/jobs?q=data+analyst&l=canada\" #&start=\"\" +str(i) \n",
    "#driver=webdriver.Chrome()\n",
    "df=pd.DataFrame(columns=[\"Title\", \"Company\", \"Location\", \"Job_Type\", \"Salary\", \"Posting_Date\",\"Current_date\",\"JobLink\", \n",
    "                         \"Description\", \"Full_Job_Description\"])\n",
    "for i in range(0,500,10):\n",
    "    driver.get(url)    \n",
    "\n",
    "    driver.implicitly_wait(4)\n",
    "    all_jobs=driver.find_elements(By.CLASS_NAME,value='job_seen_beacon')\n",
    "    for job in all_jobs:\n",
    "            result_html=job.get_attribute(\"innerHTML\")\n",
    "            soup=BeautifulSoup(result_html,\"html.parser\")\n",
    "\n",
    "            try:\n",
    "                title=job.find_element(By.TAG_NAME, 'span').text \n",
    "            except:\n",
    "                title='None'\n",
    "\n",
    "            try:\n",
    "                company=soup.find(\"span\", class_=\"css-1x7z1ps eu4oa1w0\").text\n",
    "            except:\n",
    "                company='None'   \n",
    "\n",
    "            try:\n",
    "                location=soup.find(class_=\"css-t4u72d eu4oa1w0\").text.replace(\"\\n\",\" \")\n",
    "            except:\n",
    "                location='None'\n",
    "\n",
    "            try:\n",
    "                job_type=soup.find(class_=\"css-l1havw2 eu4oa1w0\").text.replace(\"\\n\",\" \")\n",
    "            except:\n",
    "                job_type='None'\n",
    "\n",
    "            try:\n",
    "                salary=soup.find(\"div\",class_=\"metadata salary-snippet-container\").text.replace(\"\\n\",\" \")\n",
    "            except:\n",
    "                salary='None'\n",
    "\n",
    "            try: \n",
    "                postdate = soup.find('span','date').text\n",
    "            except:\n",
    "                postdate='None'\n",
    "\n",
    "            currentdate=datetime.today().strftime(\"%Y-%m-%d\")\n",
    "                            \n",
    "            link=soup.find('a',class_='jcs-JobTitle').get('href')\n",
    "            joblink = 'https://ca.indeed.com' + link\n",
    "            try:\n",
    "                description=soup.find(class_=\"job-snippet\").text.replace(\"\\n\",\" \")\n",
    "            except:\n",
    "                description='None'  \n",
    "\n",
    "            #job_response = requests.get(joblink)\n",
    "            #job_data = job_response.text\n",
    "            #job_soup = BeautifulSoup(job_data, 'html.parser')\n",
    "            #print(job_soup)\n",
    "            #descriptionlink = job.find_element_by_tag_name('a').get('href')\n",
    "            concatdescriptionlink = description\n",
    "\n",
    "            driver.get(concatdescriptionlink)\n",
    "            job_description_element = concatdescriptionlink.find_element_by_id('jobDescriptionText')\n",
    "            job_description = job_description_element.text.strip()\n",
    "            descriptions.append(job_description)\n",
    "\n",
    "\n",
    "            try:\n",
    "                    WebDriverWait(driver, 5).until(EC.visibility_of_element_located(\n",
    "                            (By.CSS_SELECTOR, \"button.popover-x-button-close.icl-CloseButton\"))).click()\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "            job_description_tag = job_soup.find('div',{'id':'jobDescriptionText'})\n",
    "            full_job_description = job_description_tag.text if job_description_tag else \"N/A\"    \n",
    "\n",
    "            df=df.append({\"Title\":title,\"Company\":company,\"Location\":location, \"Job_Type\":job_type, \"Salary\":salary,\"Posting_Date\":postdate,\n",
    "                         \"Current_date\":currentdate,\"Description\":description, \"Full_Job_Description\":full_job_description}, ignore_index=True)\n",
    "            time.sleep(15)  \n",
    "\n",
    "    df.to_csv(\"df1.csv\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afad99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
